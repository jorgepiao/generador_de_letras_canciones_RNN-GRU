{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fE_HiTeirfRi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ECr2yuorfR7"
      },
      "source": [
        "## Cargar el Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbMI7tyjrfR-",
        "outputId": "f651869e-aae7-430d-cffc-ab351883323f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de caracteres en el corpus:  2333941\n",
            "Primeros 100 caracteres del corpus:\n",
            " en la improvisación\n",
            "lamentablemente yo muerdo como león\n",
            "yo te prendo fuego, el concepto es juego\n",
            "y c\n"
          ]
        }
      ],
      "source": [
        "archivo = 'lyrics_data_esp.txt' # lyrics\n",
        "\n",
        "# guardar el archivo (corpus) en una variable\n",
        "texto = open(archivo, 'rb').read()\n",
        "texto = texto.decode(encoding='utf-8')\n",
        "\n",
        "# imprimir numero de caracteres del corpus\n",
        "# y los primeros 100 caracteres\n",
        "print('Total de caracteres en el corpus: ', len(texto))\n",
        "print('Primeros 100 caracteres del corpus:\\n', texto[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj9C07OQrfSO"
      },
      "source": [
        "## Vectorizar el texto  \n",
        "\n",
        "• Dar un número de índice a cada carácter único.  \n",
        "• Ejecutar un ciclo for en el corpus e indexar cada carácter en todo el texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyr-kXB3rfSf",
        "outputId": "eb2049a8-a2e6-4ed6-8bf2-489e95f39f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caracteres unicos en el corpus:  73\n",
            "Parte del set de caracteres:\n",
            " ['\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', ',']\n"
          ]
        }
      ],
      "source": [
        "# extraer los caracteres unicos en el corpus\n",
        "vocab = sorted(set(texto))\n",
        "print('Caracteres unicos en el corpus: ', len(vocab))\n",
        "print('Parte del set de caracteres:\\n', vocab[:10])\n",
        "\n",
        "# dar a cada caracter un numero de indice\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "# copiar los elementos del conjunto único a una matriz NumPy para su uso posterior\n",
        "# en la decodificación de las predicciones\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# vectorizar el texto con un bucle for simple donde revisamos cada carácter del\n",
        "# texto y asignamos su valor de índice correspondiente y guardamos todos los\n",
        "# valores de índice como una nueva lista\n",
        "texto_a_int = np.array([char2idx[c] for c in texto])\n",
        "\n",
        "# print('\\n',char2idx)\n",
        "# print('\\n',idx2char)\n",
        "# print('\\n',texto_a_int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki1VJK4krfS_"
      },
      "source": [
        "## Crear el Dataset  \n",
        "\n",
        "El metodo from_tensor_slices del modulo Dataset crea un objeto TensorFlow Dataset a partir de nuestro objeto texto_a_int, y los dividiremos en lotes. La longitud de cada entrada del conjunto de datos esta limitada a 100 caracteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wfrsnCJrfTL",
        "outputId": "f5e38b19-fa61-47b3-a712-7680ddf3a2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset shapes: (101,), types: tf.int64>\n"
          ]
        }
      ],
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(texto_a_int)\n",
        "seq_tam = 100   # El maximo para una entrada unica\n",
        "secuencias = char_dataset.batch(seq_tam+1, drop_remainder=True)\n",
        "\n",
        "print(secuencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eygPjQAtrfTQ"
      },
      "source": [
        "El objeto 'secuencias' contiene secuencias de caracteres, pero crearemos una tupla de estas secuencias simplemente para alimentar el modelo RNN conla funcion map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg7kQ_EfrfTW",
        "outputId": "91de1ed7-b798-43f5-985f-d90d0f9af7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = secuencias.map(split_input_target)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgUMwgynrfTa"
      },
      "source": [
        "Mezclar el conjunto de datos y lo dividirlo en lotes de 64 oraciones (sentencias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbwDNXbHrfTc",
        "outputId": "c7c7fa99-7412-466e-a18d-2690f3146b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IalPzcDGrfTf"
      },
      "source": [
        "## Construir el modelo  \n",
        "\n",
        "Construir el modelo de manera que acepte 64 oraciones de entrada a la vez. Despues de entenar el modelo, ingresaremos oraciones individuales para generar nuevas. Por lo tanto, necesitamos diferentes tamaños de lotes para los modelos de entrenamiento previo y posterior.\n",
        "\n",
        "Para esto, crearemos una función que nos permita reproducir modelos para diferentes tamaños de lote."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDxXc72irfTg"
      },
      "source": [
        "Hay tres capas en nuestro modelo:  \n",
        "\n",
        "• Una capa Embedding: esta capa sirve como capa de entrada, acepta valores de entrada (en formato numerico) y los convierte en vectores.  \n",
        "• Una capa GRU: una capa RNN llena con 1024 unidades de descenso de gradiente.  \n",
        "• Una capa densa (Dense): para generar el resultado, con salidas vocab_size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gqGh2YpWrfTj"
      },
      "outputs": [],
      "source": [
        "def construir_modelo(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "        \n",
        "        tf.keras.layers.Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim,\n",
        "            batch_input_shape=[batch_size, None]),\n",
        "        \n",
        "        tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer='glorot_uniform'),\n",
        "        \n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqCbv0TvrfTp"
      },
      "source": [
        "#### Modelo para Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyxDsT0srfTq",
        "outputId": "dfd85175-b289-4a38-edaa-9f45511a89a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           18688     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 73)            74825     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,031,817\n",
            "Trainable params: 4,031,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = construir_modelo(\n",
        "    vocab_size = len(vocab), # no. of uniques characters\n",
        "    embedding_dim = embedding_dim, # 256\n",
        "    rnn_units = rnn_units, # 1024\n",
        "    batch_size = BATCH_SIZE) # 64 for the training\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBe4bPKprfTs"
      },
      "source": [
        "## Compilar y entrenar el modelo  \n",
        "\n",
        "Optimizador: Adam\n",
        "Funcion de perdida: sparse categorical crossentropy\n",
        "\n",
        "Vectorizamos nuestro texto como números enteros (p. Ej., [0], [2], [1]), no en formato one-hot (p. Ej., [0,0,0], [0,1,], [1 , 0,0]). Para poder generar números enteros, debemos usar una función de entropía cruzada categórica dispersa (sparse categorical crossentropy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K11Ej_2vrfTt"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LGZOzJIrfTv"
      },
      "source": [
        "Cargar los pesos y guardar el rendimiento de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ntFlCCstrfTw"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoint will be saved\n",
        "checkpoint_dir = './checkpoints'\n",
        "\n",
        "# Name of the chekpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlaXNyS1rfTx"
      },
      "source": [
        "### Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVoTkq0srfTy",
        "outputId": "95ae1b8b-0383-4bb9-cb28-eb3e19d5b6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "361/361 [==============================] - 58s 141ms/step - loss: 2.3067\n",
            "Epoch 2/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 1.7582\n",
            "Epoch 3/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 1.5594\n",
            "Epoch 4/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 1.4490\n",
            "Epoch 5/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 1.3703\n",
            "Epoch 6/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 1.3032\n",
            "Epoch 7/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 1.2436\n",
            "Epoch 8/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 1.1880\n",
            "Epoch 9/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 1.1378\n",
            "Epoch 10/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 1.0916\n",
            "Epoch 11/30\n",
            "361/361 [==============================] - 52s 139ms/step - loss: 1.0519\n",
            "Epoch 12/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 1.0169\n",
            "Epoch 13/30\n",
            "361/361 [==============================] - 52s 139ms/step - loss: 0.9875\n",
            "Epoch 14/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 0.9634\n",
            "Epoch 15/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 0.9434\n",
            "Epoch 16/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 0.9276\n",
            "Epoch 17/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 0.9155\n",
            "Epoch 18/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 0.9050\n",
            "Epoch 19/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 0.8970\n",
            "Epoch 20/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 0.8916\n",
            "Epoch 21/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 0.8880\n",
            "Epoch 22/30\n",
            "361/361 [==============================] - 52s 139ms/step - loss: 0.8845\n",
            "Epoch 23/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 0.8834\n",
            "Epoch 24/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 0.8843\n",
            "Epoch 25/30\n",
            "361/361 [==============================] - 52s 140ms/step - loss: 0.8851\n",
            "Epoch 26/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 0.8861\n",
            "Epoch 27/30\n",
            "361/361 [==============================] - 53s 141ms/step - loss: 0.8904\n",
            "Epoch 28/30\n",
            "361/361 [==============================] - 52s 141ms/step - loss: 0.8933\n",
            "Epoch 29/30\n",
            "361/361 [==============================] - 53s 142ms/step - loss: 0.8963\n",
            "Epoch 30/30\n",
            "361/361 [==============================] - 53s 142ms/step - loss: 0.9034\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg2pLrcJvvRZ"
      },
      "source": [
        "## Generando texto con el modelo entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XiHU5mK6rfT4",
        "outputId": "36fd55b3-f365-4745-a514-e2c2a711eb10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./checkpoints/ckpt_30'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Ver la ubicación de nuestro último checkpoint\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8jEWThE7dJ9",
        "outputId": "873a14d0-4cb6-4096-c326-d5e0a4f15d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            18688     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 73)             74825     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,031,817\n",
            "Trainable params: 4,031,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "# Ver la informacion del modelo\n",
        "model = construir_modelo(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.save('gru_model_lyrics.h5') # Guardar el modelo completo\n",
        "model.build(tf.TensorShape([1,None]))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAG6a85U88oE"
      },
      "source": [
        "Funcion personalizada para preparar nuestra entrada para el modelo. Tenemos que configurar lo siguiente:  \n",
        "\n",
        "• La cantidad de caracteres a generar.  \n",
        "• Vectorizar la entrada (de cadena a numeros).  \n",
        "• Una variable vacia para almacenar el resultado.  \n",
        "• Un valor de temperatura para ajustar manualmente la variabilidad de las predicciones.  \n",
        "• Desvectorizar la salida y tambien ingresar la salida al modelo nuevamente para la proxima prediccion.  \n",
        "• Unir todos los caracteres generados para tener una cadena final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "60ma6ix-8JjU"
      },
      "outputs": [],
      "source": [
        "def generar_texto(model, num_generate, temperature, start_string):\n",
        "    input_eval = [char2idx[s] for s in start_string]   # de string a numero (vectorizacion)\n",
        "    input_eval = tf.expand_dims(input_eval, 0)   # dimension\n",
        "    texto_generado = []   # array vacio para guardar los resultados\n",
        "    model.reset_states()   # limpiar los estados ocultos de la RNN\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predicciones = model(input_eval)   # prediccion para un solo caracter\n",
        "        predicciones = tf.squeeze(predicciones, 0)  # remover el batch\n",
        "\n",
        "        # usar una distribucion categorica para predecir el caracter devuelto por el modelo\n",
        "        # una temperatura mas alta aumenta la probabilidad de seleccionar un caracter menos probable\n",
        "        # mas bajo --> mas predecible\n",
        "        predicciones = predicciones / temperature\n",
        "        prediccion_id = tf.random.categorical(predicciones, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # El caracter predicho como la siguiente entrada al modelo\n",
        "        # junto con el estado oculto anterior\n",
        "        # Entonces el modelo hace la próxima prediccion basada en el caracter anterior\n",
        "        input_eval = tf.expand_dims([prediccion_id], 0)\n",
        "        # Desvectorizar el numero y agregar al texto generado\n",
        "        texto_generado.append(idx2char[prediccion_id])\n",
        "\n",
        "    return (start_string + ''.join(texto_generado))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a85lfUvHDHdq"
      },
      "source": [
        "Generar un texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2DMqUBWDJ7n",
        "outputId": "3a19db4c-18c7-42ea-a2af-3c78a3417212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fuego, que elegíder, te gana en blanca\n",
            "saben que en esto yo lo mato a él\n",
            "las perfectos\n",
            "sobran ganas de rra me estrfeca\n",
            "putas de freez, yeah\n",
            "\n",
            "muerte parece mejor que me aman, he vida, más que amar\n",
            "pero yo, lo quiero de ver el sol se están encerrados una cuerpo\n",
            "y ya no tranco, los quieren moverte le exploque un paracias, ah\n",
            "\n",
            "\n",
            "todo te la click de ese video', salgo con un pare' una de la pared\n",
            "me va a gano hasta abajo buscando escándalo en el radio me han pegado que me digan qué pudieron\n",
            "no puede quejar,\n"
          ]
        }
      ],
      "source": [
        "texto_generado = generar_texto(\n",
        "                model,\n",
        "                num_generate=500,\n",
        "                temperature=1,\n",
        "                start_string=u\"fuego\")\n",
        "\n",
        "print(texto_generado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_LH8RTTUXJL"
      },
      "source": [
        "## Modelo usando los pesos guardados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsQcmNNNDlDV",
        "outputId": "c8c2d5bc-fd69-475a-c644-3f33ba55fc13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (64, None, 256)           18688     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (64, None, 73)            74825     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,031,817\n",
            "Trainable params: 4,031,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Nuevo modelo\n",
        "\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model2 = construir_modelo(\n",
        "    vocab_size = len(vocab), # no. of uniques characters\n",
        "    embedding_dim = embedding_dim, # 256\n",
        "    rnn_units = rnn_units, # 1024\n",
        "    batch_size = BATCH_SIZE) # 64 for the training\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "auscwIq3UXJO"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model2.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ8EH_pJUXJQ",
        "outputId": "84c130af-9331-4607-9cd1-c160ed3e2d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            18688     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 73)             74825     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,031,817\n",
            "Trainable params: 4,031,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Hacer predicciones con el nuevo modelo\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model2 = load_model('gru_model_lyrics.h5')\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Og2hgyGUXJR",
        "outputId": "faadc6fa-e6e8-400a-8bf0-aa888d5d579e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fuego\n",
            "estoy yendo pa' la cabeza\n",
            "\n",
            "el barrio no puede ocupar mí llamada amaro, fue un encanto\n",
            "me extraño porque dando suivan \n",
            "mirá como falta la sensei\n",
            "\n",
            "\n",
            "me peo, éstropara en la escena virgen\n",
            "\n",
            "ento a donde soy invicto, con un tembo de barrio\n",
            "bardendo a todos mierda para que conoce\n",
            "\n",
            "no se me pone quejos, tanto lugar suficiente\n",
            "iba calle por donde aguanta cuando ataco lo pese'\n",
            "\n",
            "yeh\n",
            "yeh, yeh\n",
            "yeh, yeh xanax, siempre ahora le hago confundo son roethow\n",
            "\n",
            "to you keep limina\n",
            "yeah code, code mí, dodel bebebaty\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "# Generar texto antes de cargar los pesos\n",
        "texto_generado = generar_texto(\n",
        "                model2,\n",
        "                num_generate=500,\n",
        "                temperature=1,\n",
        "                start_string=u\"fuego\")\n",
        "\n",
        "print(texto_generado)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PujKgFTprUHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text_generation_NLP_GRU.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}